<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paul Kelly&#39;s</title>
    <description>Fabulous Adventures In Software</description>
    <link>http://pkelly03.github.io</link>
    <atom:link href="http://pkelly03.github.io/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Book Selection - Microservices Architecture</title>
        <description>&lt;p&gt;Chapter 1- Key benefits&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Technology Heterogeneity - pick the right tool for the right job&lt;/li&gt;
  &lt;li&gt;Resilience - the notion of a bulkhead, if one component fails, then the failure doesn’t cascade, on a monolithic service, if the service fails, everything fails.&lt;/li&gt;
  &lt;li&gt;Improved scaling - with smaller services, you can scale those services that need scaling&lt;/li&gt;
  &lt;li&gt;Ease of deployment - make a change to an individual service and deploy it independently&lt;/li&gt;
  &lt;li&gt;Smaller teams working on smaller services tend to be more efficient&lt;/li&gt;
  &lt;li&gt;Individual service being so small, the cost of replacement is tiny in comparison to a larger monolithic app&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Chapter 2- Principles/Practices role of an architect&lt;/p&gt;

&lt;p&gt;Think of an architect like a town planner, different zones, you should care less about what happens between the zones/service boundaries. Need to spend a lot of time thinking about how the services talk to each other. Important to pick principles/practices that your team sticks too.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Example principles might be (consistent interfaces between services, no silver bullets, make choices that favor rapid feedback and change)&lt;/li&gt;
  &lt;li&gt;Example practices might be (standard HTTP/Rest, Standard monitoring service (nagios etc) Encapsulate legacy, Published integration model)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Questions: Have never done this before, has anyone else you know?&lt;/p&gt;

&lt;p&gt;Chapter 3 - What makes a good service?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Loosely coupled - make a change to a service and deploy independent of other services&lt;/li&gt;
  &lt;li&gt;Strong cohesion - related behaviour to sit together (same zone), if we have to make changes to behaviour and it effects lots of different services, not good. Good to find boundaries in our problem domain that helps ensure related behaviour in one place - leads to chatty apps.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Questions: We don’t do this in DCJ, why?&lt;/p&gt;

&lt;p&gt;Bounded Context&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;From DDD, the idea that a given problem domain consists of multiple bounded contexts. Within a bounded contexts, there are things that communicated within the boundary, and thingNbcbchocb b b Khulna bank s (models) that communicate outside. Shared externally. &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Questions/Discussion: How to you go about implementing bounded contexts? He suggests not by data that is shared, but by business capability. 
Answer: He suggests starting off with coarse grained services, sub divide y into nested contexts if required.
Discussion:&lt;/p&gt;

&lt;p&gt;Chapter 4 - Integration&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Introduce a service to talk to the database rather than the external apps talking directly to the database.&lt;/li&gt;
  &lt;li&gt;Talks a lot about different technologies, RPC (SOAP/RMI), HTTP(REST), HATEOAS, JSON vs XML.&lt;/li&gt;
  &lt;li&gt;Async, he recommends wrap an overall interaction which is asynchronous into a bunch of synchronous calls. Discussion point below.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Discussion - Above, HATEOAS over REST. Chatty apps.
Discussion - (Pg 66) Lifecycle of key domain events
Discussion - wrap an overall interaction which is asynchronous into a bunch of synchronous calls. Discuss. Event models with synchronous rest over http can be a very powerful combination&lt;/p&gt;

&lt;p&gt;Chapter 5 - Integratating with 3rd Party software&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Nuno did this, they had a micro service that acted as a nice facade to a 3rd party system.&lt;/li&gt;
  &lt;li&gt;This facade/service should be split/placed in the bounded context that makes sense. ie Employee or Payments&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Chapter 6 - Breaking the monolith/legacy&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Identify what high level bounded contexts exists with the system/organisation&lt;/li&gt;
  &lt;li&gt;Create package/modules based on these bounded contexts.&lt;/li&gt;
  &lt;li&gt;What seam do you start with first? lots of factors, technology choice, pace of change&lt;/li&gt;
  &lt;li&gt;Talks a lot about breaking up databases which i didn’t really investigate&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Chapter 7 - Deployment&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Recommends a single CI build per micro service&lt;/li&gt;
  &lt;li&gt;One of the big takes from this book is that he stresses that we should be able to deploy our services independent of each other.&lt;/li&gt;
  &lt;li&gt;Various types of artefacts that we can move across pipeline to production these include:&lt;/li&gt;
  &lt;li&gt;
    &lt;ul&gt;
      &lt;li&gt;Platform specific artefacts: use configuration management tools like puppet and chef, downside is the time it might take to run scripts on a machine (in AWS, download and install java, mysql etc -  prob 10 mins) especially in on demand computing platforms, complex deployment scripts &lt;/li&gt;
      &lt;li&gt;Operating system artifacts: rpm packages, msi etc. use tools native to OS to install it. upside, simplified development approach. &lt;/li&gt;
      &lt;li&gt;Custom images: build image once, launch copies, no need to install dependencies. Down side, can take long to build an image. Large image sizes over a slow wifi connection.&lt;/li&gt;
      &lt;li&gt;Images as artefacts: ie Netflix AWS AMIs, pro’s easy way to implement an immutable server.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Immutable servers, not allowing anyone to make config changes to a box once an image has been built. If config changes differ to whats in source control, called configuration drift. Steps around this, we can disable SSH.&lt;/li&gt;
  &lt;li&gt;Continuous delivery, moving through the pipelines, problems can occur if environments differ greatly. multiple load balanced hosts versus a single stand alone laptop. You want to ensure that your environments are more production like to catch issues (i.e. session replication etc), although there is a balance between time to reproduce production environment and speed of development and feedback time.&lt;/li&gt;
  &lt;li&gt;Differing artefacts across different environments, i.e. Customer-service-test artefact, and customer-service-prod artefact, how can we be sure that we have verified the software that ends up in production. &lt;/li&gt;
  &lt;li&gt;Better to use a single artefact and manage configuration separately. &lt;/li&gt;
  &lt;li&gt;Multiple services per host - while cost efficient has downsides: &lt;/li&gt;
  &lt;li&gt;
    &lt;ul&gt;
      &lt;li&gt;limit your deployment options, image based deployments and immutable servers (unless we tie various services together in a single artefact which we really don’t want to do)&lt;/li&gt;
      &lt;li&gt;Want to be able to deploy each service independently.&lt;/li&gt;
      &lt;li&gt;Sometimes end up treating different services with different needs the same (some might requrie a faster box, others more scaling options)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Using application containers (mostly downsides), 5 java services in 1 servlet container (and 1 jvm) versus 5 jvms, while this is nice the downsides outweigh this:&lt;/li&gt;
  &lt;li&gt;
    &lt;ul&gt;
      &lt;li&gt;Lots offer shared in memory state which is something we really want to avoid as affects scaling our services&lt;/li&gt;
      &lt;li&gt;slow spin up times&lt;/li&gt;
      &lt;li&gt;netty better.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Single service per host&lt;/li&gt;
  &lt;li&gt;
    &lt;ul&gt;
      &lt;li&gt;monitoring and remediation much easier.&lt;/li&gt;
      &lt;li&gt;single point of failure, does not affect multiple services&lt;/li&gt;
      &lt;li&gt;more hosts to manage though&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Platform as a service&lt;/li&gt;
  &lt;li&gt;
    &lt;ul&gt;
      &lt;li&gt;Very useful, think heroku, handles scaling etc.&lt;/li&gt;
      &lt;li&gt;not great if you have a non standard application&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;Physical to virtual&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Traditional world of virtualization&lt;/li&gt;
  &lt;li&gt;
    &lt;ul&gt;
      &lt;li&gt;Allows us to slice up a physical server into separate hosts&lt;/li&gt;
      &lt;li&gt;Sock drawer analogy, dividers take up space also. &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Vagrant&lt;/li&gt;
  &lt;li&gt;
    &lt;ul&gt;
      &lt;li&gt;provides you with a virtual cloud on your laptop&lt;/li&gt;
      &lt;li&gt;Makes it easier for you to create production like environments on your local machine.&lt;/li&gt;
      &lt;li&gt;Spin up multiple vms, have them mapped to individual directories, makes changes to files and see changes reflected directly&lt;/li&gt;
      &lt;li&gt;Downside, running lots of VMs will tax the average developer machine.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Docker&lt;/li&gt;
  &lt;li&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Chapter 8 testing &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use consumer driven contracts instead of end to end tests&lt;/li&gt;
  &lt;li&gt;What’s wrong with end to end tests, can be brittle and flaky, people lose trust quickly.&lt;/li&gt;
  &lt;li&gt;Martin fowler talks about test double as a substitute for spy’s mocks stubs etc. read his post &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Chapter 9 monitoring &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Correlation ids is a useful concept, where you pass this along to all services. &lt;/li&gt;
  &lt;li&gt;Graphite seemed like a nice too, simple Ali for generating charts of your service &lt;/li&gt;
  &lt;li&gt;Ssh multiplexors allow you to perform the same command on multiple ssh terminals.&lt;/li&gt;
  &lt;li&gt;Use metrics to see what services/features are being used.&lt;/li&gt;
  &lt;li&gt;Hysterix is a circuit breaking library &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Chapter 10 security &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Types&lt;/li&gt;
  &lt;li&gt;
    &lt;ul&gt;
      &lt;li&gt;HTTP Basic Authentication - username password sent as part of HTTP header - highly problematic as username and password not sent in a secure manner, should only be used over HTTPS&lt;/li&gt;
      &lt;li&gt;HTTP Basic Authentication over HTTPS - Due to nature of https, the client gains strong guarantees that the server it is talking to is who it thinks it is. Challenge is that each server needs to manage it’s own certificates which is a problem when shared over multiple machines, certificate issuing process but an operational burden.&lt;/li&gt;
      &lt;li&gt;TLS - Transport Layer Security. Used to verify that the client is who we think we are talking to (unlike HTTPS). Each client has an X.509 certificate installed, used to establish a link between client and server. &lt;/li&gt;
      &lt;li&gt;HMAC over HTTP - Body request along with key is hashed.Request uses its own copy of the private key and the request body to recreate the hash, if it matches then it allows the request. Man in the middle messes with  the request then the hash won’t match.Also, private key is never sent in transit, so cannot be compromised. Downside is that the client and server need a shared secret. Amazon use this for S3, if use it, use a long key SHA-256.&lt;/li&gt;
      &lt;li&gt;API Keys - Not security as such, but more to protect services for others. Using an issued API key, twitter can limit how many requests a given person has, stopping overload. Amazon use API keys to limit what services a user has.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Where do we store our keys? One option is to use a key vault which your server can access when it needs a key.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Chapter 11 Conways law and system design&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Geographical boundaries between people involved with the development of a system can be a great way to drive when services can be decomposed. This problem came from when one service was worked on my 2-3 geolocated teams. Nightmare, fine grained communication lost. Align service ownership to co located teams, which themselves are aligned around the same bounded contexts of the organisation.&lt;/li&gt;
  &lt;li&gt;Internal open source - one solution to service ownership. Instead people submit changes for acceptance. Need a core team of committers (gatekeepers)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Chapter 12 Microservices at scale&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Failure is everywhere. Many organisations try to put processes and controls to try and stop failure from occurring. But put little or no-thought into actually making it easier to recover from failure in the first place.&lt;/li&gt;
  &lt;li&gt;Degrading functionality - Sample page on an ecommerce site, we might use 3-4 services, basket, catalogue, &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 20 Mar 2015 00:00:00 +0000</pubDate>
        <link>http://pkelly03.github.io/book-selection---microservices-architecture</link>
        <guid isPermaLink="true">http://pkelly03.github.io/book-selection---microservices-architecture</guid>
      </item>
    
      <item>
        <title>Reactive Design Patterns - Skimmers Guide Chapter 1</title>
        <description>&lt;ul&gt;
  &lt;li&gt;Outlines the importance of responsiveness to users. Books will give tips how to stay responsive in the face of variable load, partial outages and program failure. &lt;/li&gt;
  &lt;li&gt;Presents the reactive manifesto which is a small summary of the challenges facing computer systems. 
    &lt;ol&gt;
      &lt;li&gt;It must react to its users (responsiveness)	&lt;/li&gt;
      &lt;li&gt;It must react to failure and stay available (resilience)	&lt;/li&gt;
      &lt;li&gt;It must react to variable load conditions (scalability)	&lt;/li&gt;
      &lt;li&gt;It must react to events (event orientation)	&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Responsiveness - must react to users.
    &lt;ul&gt;
      &lt;li&gt;Old way, consider val x*  = f(30) - evaluation of the function occurs synchronously. Falls over when it needs to be distributed among multiple cores etc.&lt;/li&gt;
      &lt;li&gt;In 94, used to be a huge difference between local and remote calls.&lt;/li&gt;
      &lt;li&gt;Major advances in networking hardware. No longer the case, a remote call is almost the same. Need to treat remote and local calls the same.&lt;/li&gt;
      &lt;li&gt;To be responsive, we need the latency to be as small as possible. We need to define an upper limit on this latency, why? it allows users to cap their wait times accordingly.&lt;/li&gt;
      &lt;li&gt;Parrallization - 3 sub tasks executed sequentially, total response is sum of all 3 latencies. 3 sub tasks executed in parallel, the total response is the time to complete the slowest of the 3 sub tasks.&lt;/li&gt;
      &lt;li&gt;Sequential way&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;code&gt;
   	val response1 = getVal1()
   	val response2 = getVal2()
   	val response3 = getVal3()
   	def compose(response1, response2, response3)
  &lt;/code&gt;
  This way is blocking. 
  A better way would be when the last service completes its job it organises dispatching the event.Think of it as the last sub task that comes back dispatches the event.
  Better way is through &lt;code&gt;CompletableFuture&lt;/code&gt; or the scala way as below:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; val fa: Future[ReplyA] = taskA()
	val fb: Future[ReplyB] = taskB()
	val fc: Future[ReplyC] = taskC()
    
 val fr: Future[Result] = for (a &amp;lt;- fa; b &amp;lt;- fb; c &amp;lt;- fc)
                          yield aggregate(a, b, c)
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;How do you choose best case latency? 3 mins 16 is too long, 50 ms sounds good, but 1 in 100 requests might never go through even though the service is working.&lt;/li&gt;
  &lt;li&gt;This is basically saying that choosing the upper latency bound is always going to be a tradeoff between reliability and responsiveness.&lt;/li&gt;
  &lt;li&gt;Bounding latency, can use bounded queues. Reqeusts coming in faster then we can process responses. In 1s we receive 110 requests, we process 100. This leads to an extra 0.1s of latency. Latency can quickly add up. Use explicit queues. Kind of confusing this section [REVISIT]&lt;/li&gt;
  &lt;li&gt;Circuit Breakers - interest concept. When a service is being overwhelmed, and the time is consistently rising above the threshold. The circuit breaker will trip and requests will now take a different route - degraded service or failing fast etc. When it has time to recuperate, circuit breaker will resume as normal allowing requests to flow correctly through teh system.&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;React to failure
    &lt;ul&gt;
      &lt;li&gt;Systems will go down, software, hardware, human. It’s only a matter of when.&lt;/li&gt;
      &lt;li&gt;Install bulkheads to compartments to isolate failure, taken from shipbuilding. One container will fill with water, all others will be fine.&lt;/li&gt;
      &lt;li&gt;Manifesto calls it resilience instead of reliability as it’s about how quickly the service(s) can bounce back from a production &lt;issue class=&quot;&quot;&gt;&lt;/issue&gt;&lt;/li&gt;
      &lt;li&gt;Supervisor role, instead of coupling fault tolerance with business logic, seperate out the fault tolerance section.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reacting to load
    &lt;ul&gt;
      &lt;li&gt;To react to load, a system needs to do be able to do 2 things:
        &lt;ol&gt;
          &lt;li&gt;Need to split up individual streams of work items that can be worked on on different machines in parallel.&lt;/li&gt;
          &lt;li&gt;Route traffic in the direction of another service depending on load, or spin up new instances if reaching capactity or wind down services if very quiet. &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;number of requests serviced by the system = average number of requests * time spent processing. There is no reason why we can’t automate this figure. This will allow us to determine how much we should parallelize. We can capture requests at entry and exit. &lt;/li&gt;
      &lt;li&gt;This service would act like a supervisor or monitoring service that polls the system and gathers this metric to determine whether it needs to scale up.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reacting to events
    &lt;ul&gt;
      &lt;li&gt;The focus should be on events instead of method calls. &lt;/li&gt;
      &lt;li&gt;Should focus on high level interactions instead of micromanaged getters/setters that promote tight coupling etc.&lt;/li&gt;
      &lt;li&gt;Amdahls Law, if a program can be 95% parallelizable, then 5% must be done processed in a globally agreed sequential fashion. Amdahls law determines the effect this has on a system. This limits scalability. Ideally you want to share nothing!&lt;/li&gt;
      &lt;li&gt;Most APIs ie socket api, have a synchronous facade above an event-driven system. Blocking calls lead to expensive context switching between threads. &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;How does this change the way we program?
  * Strive for bulkheads, where 1 service if it fails, it fails in isolation, service chat through asynchronous messaging.
  * Need to treat all interactions as distributed even if the run on the same core. 
  * Lose consistency in the CAP theorem, Almost scary - seems like a shift away from strong transactional guarantees. Two people editing a document, if they were both to be consistent, then each keypress would need to register with a central server to determine any updates before displaying the typed character. Imagine the latency? How would this scale to 1000 users of the same document. Not very well.
  * Distributed systems are built on a new set of principles -&amp;gt; BASE 
    1. Basic Availability
	1. Soft State - State needs to be maintained actively rather than persisted by default.
	1. Eventual Consistency - possible for external observers to see data which is inconsistent.
  * Similar to SOA patterns? Kind of, but need to move away from the synchronous blocking style of communication and leveraging the event driven nature of the underlying system. 
  * Essential complexity - complexity introduced by the problem domain
  * Incidental complexity - complexity introduced by the solution to the problem.
  * Higher level abstractions might tackle the essential complexity, but introduce incidental complexity  - ie the performance and scalability issues that might arise.
  * &lt;/p&gt;
</description>
        <pubDate>Mon, 11 Aug 2014 00:00:00 +0100</pubDate>
        <link>http://pkelly03.github.io/reactive-design-patterns--chapter-1-digest</link>
        <guid isPermaLink="true">http://pkelly03.github.io/reactive-design-patterns--chapter-1-digest</guid>
      </item>
    
      <item>
        <title>Book Selection - Reactive Design Patterns</title>
        <description>&lt;p&gt;Our company set up a book club recently where our loose plan is that we are going to try and read an IT book once a month. This month we decided on Reactive Design Patterns. Will keep you posted how we get on. &lt;/p&gt;

</description>
        <pubDate>Fri, 25 Jul 2014 00:00:00 +0100</pubDate>
        <link>http://pkelly03.github.io/reactive-design-patterns-chapter1-summary</link>
        <guid isPermaLink="true">http://pkelly03.github.io/reactive-design-patterns-chapter1-summary</guid>
      </item>
    
  </channel>
</rss>