<ul>
  <li>Outlines the importance of responsiveness to users. Books will give tips how to stay responsive in the face of variable load, partial outages and program failure. </li>
  <li>Presents the reactive manifesto which is a small summary of the challenges facing computer systems. 
    <ol>
      <li>It must react to its users (responsiveness)	</li>
      <li>It must react to failure and stay available (resilience)	</li>
      <li>It must react to variable load conditions (scalability)	</li>
      <li>It must react to events (event orientation)	</li>
    </ol>
  </li>
</ul>

<ol>
  <li>Responsiveness - must react to users.
    <ul>
      <li>Old way, consider val x = f(30) - evaluation of the function occurs synchronously. Falls over when it needs to be distributed among multiple cores etc.</li>
      <li>In 94, used to be a huge difference between local and remote calls.</li>
      <li>Major advances in networking hardware. No longer the case, a remote call is almost the same. Need to treat remote and local calls the same.</li>
      <li>To be responsive, we need the latency to be as small as possible. We need to define an upper limit on this latency, why? it allows users to cap their wait times accordingly.</li>
      <li>Parrallization - 3 sub tasks executed sequentially, total response is sum of all 3 latencies. 3 sub tasks executed in parallel, the total response is the time to complete the slowest of the 3 sub tasks.</li>
      <li>Sequential way</li>
    </ul>

    <p><code>
   	val response1 = getVal1()
   	val response2 = getVal2()
   	val response3 = getVal3()
   	def compose(response1, response2, response3)
  </code>
  This way is blocking. Think of it as the last sub task that comes back dispatches the event.
  Better way is through <code>CompletableFuture</code> or the scala way as below:</p>

    <pre><code> val fa: Future[ReplyA] = taskA()
	val fb: Future[ReplyB] = taskB()
	val fc: Future[ReplyC] = taskC()
    
 val fr: Future[Result] = for (a &lt;- fa; b &lt;- fb; c &lt;- fc)
                          yield aggregate(a, b, c)
</code></pre>
  </li>
</ol>

<ul>
  <li>How do you choose best case latency? 3 mins 16 is too long, 50 ms sounds good, but 1 in 100 requests might never go through even though the service is working.</li>
  <li>This is basically saying that choosing the upper latency bound is always going to be a tradeoff between reliability and responsiveness.</li>
  <li>Bounding latency, can use bounded queues. Reqeusts coming in faster then we can process responses. In 1s we receive 110 requests, we process 100. This leads to an extra 0.1s of latency. Latency can quickly add up. Use explicit queues. Kind of confusing this section [REVISIT]</li>
  <li>Circuit Breakers - interest concept. When a service is being overwhelmed, and the time is consistently rising above the threshold. The circuit breaker will trip and requests will now take a different route - degraded service or failing fast etc. When it has time to recuperate, circuit breaker will resume as normal allowing requests to flow correctly through teh system.</li>
</ul>

